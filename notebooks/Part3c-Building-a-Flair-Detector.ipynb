{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wiZA3bzvhDJc"
   },
   "source": [
    "# Part 3c - Building a flair detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R6iQuC0yNaj4",
    "outputId": "1c0047eb-8081-43ba-f8a3-7f7ecab18800"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Get the required libraries and functions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding, Layer, Dense, Input, LSTM, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcPAmLSew1V_"
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "df = pd.read_csv('balanced_data.csv')\n",
    "\n",
    "# Tokenize the posts, ie assign keys to each of the words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.title)\n",
    "X = tokenizer.texts_to_sequences(df.title)\n",
    "df['tokenized'] = X\n",
    "X = list(sequence.pad_sequences(df.tokenized, maxlen=50))\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFAPwX8C8Mea"
   },
   "outputs": [],
   "source": [
    "# Convert the flairs into keys, and get an inverse mapping as well\n",
    "\n",
    "flairs = df.groupby('flair').size().index.tolist()\n",
    "flair_to_key = {}\n",
    "key_to_flair = {}\n",
    "for key, flair in enumerate(flairs):\n",
    "  flair_to_key[flair] = key\n",
    "  key_to_flair[key] = flair\n",
    "\n",
    "df['key'] = df['flair'].apply(lambda x: flair_to_key[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wyO0d2Vk0iJ"
   },
   "outputs": [],
   "source": [
    "# Prepare train and validation sets\n",
    "\n",
    "X = np.array(X)\n",
    "Y = to_categorical(list(df.key))\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.15, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-Cog78Kh5Gr"
   },
   "source": [
    "## Attention BiLSTM\n",
    "We use an Attention BiLSTM architecture. The attention mechanism is as an improvement over the encoder decoder-based neural machine translation system in NLP. More details here: https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6u_P3yt7lRTs"
   },
   "outputs": [],
   "source": [
    "# Building a standard Attention layer\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "ZjXCIUTV8OzE",
    "outputId": "93975231-520b-4988-8ea0-58f8fac864bd"
   },
   "outputs": [],
   "source": [
    "# Preparing the embedding layer using Stanford's 100 dimensional GloVe word vectors: https://nlp.stanford.edu/projects/glove/\n",
    "# First we create a mapping from the words to their vector representation\n",
    "\n",
    "mapping = {}\n",
    "f = open('/content/gdrive/My Drive/glove.6B.100d.txt')\n",
    "for embedding in f:\n",
    "    embedding = embedding.split()\n",
    "    word = embedding[0]\n",
    "    vector = embedding[1:]\n",
    "    vector = np.asarray(vector, dtype='float32')\n",
    "    mapping[word] = vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vVWO7IKV8jLz"
   },
   "outputs": [],
   "source": [
    "# Now we create the actual embedding layer, and initialise it with the the veectors extracted\n",
    "# We also freeze the layer, ie make it non-trainable\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index)+1, 100))\n",
    "for word, idx in word_index.items():\n",
    "    vector = mapping.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_weights[idx] = vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index)+1, 100, embeddings_initializer=Constant(embedding_weights), input_length=50, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Gi4BYOqx8rSQ",
    "outputId": "3bdd6532-3877-4a86-9826-0d3e04fcaaa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Attention.call of <__main__.Attention object at 0x7f538a0e1e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Attention.call of <__main__.Attention object at 0x7f538a0e1e10>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# Preparing the model\n",
    "\n",
    "inputs = Input(shape=(50,), dtype='int32')\n",
    "x = embedding_layer(inputs)\n",
    "x = Bidirectional(LSTM(512, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Attention(50)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "outputs = Dense(14, activation='softmax')(x)\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_weights.hdf5', monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gMQZUZIw8uaa",
    "outputId": "4c2b2770-536c-442f-aceb-b83487b167fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 25695 samples, validate on 4535 samples\n",
      "Epoch 1/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 2.0660 - acc: 0.3398\n",
      "Epoch 00001: val_acc improved from -inf to 0.39846, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 72s 3ms/sample - loss: 2.0657 - acc: 0.3398 - val_loss: 1.9324 - val_acc: 0.3985\n",
      "Epoch 2/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.8259 - acc: 0.4116\n",
      "Epoch 00002: val_acc improved from 0.39846 to 0.45667, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 69s 3ms/sample - loss: 1.8263 - acc: 0.4117 - val_loss: 1.6824 - val_acc: 0.4567\n",
      "Epoch 3/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.7124 - acc: 0.4501\n",
      "Epoch 00003: val_acc improved from 0.45667 to 0.48291, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 69s 3ms/sample - loss: 1.7124 - acc: 0.4501 - val_loss: 1.5924 - val_acc: 0.4829\n",
      "Epoch 4/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.6351 - acc: 0.4722\n",
      "Epoch 00004: val_acc improved from 0.48291 to 0.50165, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 70s 3ms/sample - loss: 1.6353 - acc: 0.4720 - val_loss: 1.5309 - val_acc: 0.5017\n",
      "Epoch 5/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.5730 - acc: 0.4912\n",
      "Epoch 00005: val_acc improved from 0.50165 to 0.50937, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 69s 3ms/sample - loss: 1.5728 - acc: 0.4911 - val_loss: 1.5105 - val_acc: 0.5094\n",
      "Epoch 6/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.5148 - acc: 0.5062\n",
      "Epoch 00006: val_acc improved from 0.50937 to 0.52569, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 68s 3ms/sample - loss: 1.5145 - acc: 0.5063 - val_loss: 1.4678 - val_acc: 0.5257\n",
      "Epoch 7/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.4676 - acc: 0.5184\n",
      "Epoch 00007: val_acc improved from 0.52569 to 0.53451, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 69s 3ms/sample - loss: 1.4671 - acc: 0.5187 - val_loss: 1.4397 - val_acc: 0.5345\n",
      "Epoch 8/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.4202 - acc: 0.5332\n",
      "Epoch 00008: val_acc improved from 0.53451 to 0.53649, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 68s 3ms/sample - loss: 1.4201 - acc: 0.5331 - val_loss: 1.4474 - val_acc: 0.5365\n",
      "Epoch 9/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.3802 - acc: 0.5447\n",
      "Epoch 00009: val_acc improved from 0.53649 to 0.54201, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 69s 3ms/sample - loss: 1.3799 - acc: 0.5449 - val_loss: 1.4084 - val_acc: 0.5420\n",
      "Epoch 10/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.3230 - acc: 0.5604\n",
      "Epoch 00010: val_acc did not improve from 0.54201\n",
      "25695/25695 [==============================] - 69s 3ms/sample - loss: 1.3232 - acc: 0.5604 - val_loss: 1.4161 - val_acc: 0.5418\n",
      "Epoch 11/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.2823 - acc: 0.5742\n",
      "Epoch 00011: val_acc improved from 0.54201 to 0.54730, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 68s 3ms/sample - loss: 1.2823 - acc: 0.5741 - val_loss: 1.3983 - val_acc: 0.5473\n",
      "Epoch 12/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.2318 - acc: 0.5903\n",
      "Epoch 00012: val_acc did not improve from 0.54730\n",
      "25695/25695 [==============================] - 68s 3ms/sample - loss: 1.2319 - acc: 0.5903 - val_loss: 1.4214 - val_acc: 0.5422\n",
      "Epoch 13/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.1885 - acc: 0.6035\n",
      "Epoch 00013: val_acc improved from 0.54730 to 0.55259, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 69s 3ms/sample - loss: 1.1880 - acc: 0.6037 - val_loss: 1.4237 - val_acc: 0.5526\n",
      "Epoch 14/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.1340 - acc: 0.6197\n",
      "Epoch 00014: val_acc did not improve from 0.55259\n",
      "25695/25695 [==============================] - 68s 3ms/sample - loss: 1.1341 - acc: 0.6197 - val_loss: 1.4432 - val_acc: 0.5486\n",
      "Epoch 15/15\n",
      "25664/25695 [============================>.] - ETA: 0s - loss: 1.0926 - acc: 0.6316\n",
      "Epoch 00015: val_acc improved from 0.55259 to 0.55700, saving model to best_weights.hdf5\n",
      "25695/25695 [==============================] - 68s 3ms/sample - loss: 1.0923 - acc: 0.6318 - val_loss: 1.4375 - val_acc: 0.5570\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=64, epochs=15, validation_data=(x_val, y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nks6nsIKzKgV"
   },
   "outputs": [],
   "source": [
    "# Saving the tokenizer\n",
    "\n",
    "import pickle\n",
    "pickle_out = open(\"tokenizer.pickle\", \"wb\")\n",
    "pickle.dump(tokenizer, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUOl2HTfz0cB"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xh8s1dzEz4Sf"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "G6hZAORSTNCv",
    "outputId": "29575d0d-2bec-4e22-eab0-64ef676e9e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5570011025358325\n",
      "F1 score = 0.5686316903518523\n",
      "Precision = 0.576090558305631\n",
      "Recall = 0.5766126358008499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "preds = model.predict(x_val)\n",
    "y = np.asarray([np.argmax(line) for line in y_val])\n",
    "preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y, preds)))\n",
    "print(\"F1 score = {}\".format(f1_score(y, preds, average='macro')))\n",
    "print(\"Precision = {}\".format(precision_score(y, preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y, preds, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pef1-CaP3XqG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AttentionLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
